{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn transformers torch Pillow opencv-python-headless pyngrok\n",
        "!git clone https://github.com/X-PLUG/mPLUG-Owl.git\n"
      ],
      "metadata": {
        "id": "PGtj687sTCCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5DrcTWg6xc3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31194db-fbb3-428e-a469-791712c1e086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "from transformers import AutoModelForVision2Seq, AutoTokenizer\n",
        "\n",
        "model_name = \"/content/mPLUG-Owl\"  # path to the cloned repo\n",
        "model = AutoModelForVision2Seq.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def describe_key_elements(image):\n",
        "    inputs = tokenizer(images=image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    description = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return description\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile process_a.py\n",
        "from model import describe_key_elements\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def process_a1(image_bytes):\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    description = describe_key_elements(image)\n",
        "    return description\n",
        "\n",
        "def process_a2(image_bytes, heatmap_bytes):\n",
        "    image = np.array(Image.open(io.BytesIO(image_bytes)).convert(\"RGB\"))\n",
        "    heatmap = np.array(Image.open(io.BytesIO(heatmap_bytes)).convert(\"RGB\"))\n",
        "\n",
        "    red_channel = heatmap[:,:,0]\n",
        "    _, thresholded = cv2.threshold(red_channel, 200, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "\n",
        "    salient_elements = []\n",
        "\n",
        "    for contour in contours[:5]:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        roi = image[y:y+h, x:x+w]\n",
        "        roi_image = Image.fromarray(roi)\n",
        "        description = describe_key_elements(roi_image)\n",
        "        salient_elements.append(description)\n",
        "\n",
        "    return salient_elements\n"
      ],
      "metadata": {
        "id": "T7taBe0RNNgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27611940-1a42-46ea-f945-6c8467a23dfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing process_a.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile process_b.py\n",
        "from model import assess_cognitive_load\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "cognitive_load_definition = \"\"\"\n",
        "Cognitive Load Theory is based on the model of human information processing. This model describes memory as having three main parts: sensory, working, and long-term. Sensory memory filters out most of what is going on around us, passing select information on to our working memory for additional processing. Working memory can typically process 5-9 pieces, or chunks, of information at any given time. Our working memory either discards the information or categorizes it for storing in our long-term memory. Long-term memory stores information in structures called “schemas,” which organize information based on how we use it. The more we use these schemas, the more developed they become and the easier it is to recall them. Cognitive load refers to the amount of information our working memory can process at any given time. For educational purposes, cognitive load theory helps us to avoid overloading learners with more than they can effectively process into schemas for long-term memory storage and future recall.\n",
        "\"\"\"\n",
        "\n",
        "def process_b(image_bytes):\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    analysis = assess_cognitive_load(image, cognitive_load_definition)\n",
        "    return analysis"
      ],
      "metadata": {
        "id": "cmZ1qh3LT0hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eeac76f-22b1-403c-b34b-b5150f8ff270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting process_b.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile process_c.py\n",
        "def process_c(description_a1, salient_elements_a2, cognitive_load_b):\n",
        "    summary = {\n",
        "        \"key_elements_description\": description_a1,\n",
        "        \"salient_elements\": salient_elements_a2,\n",
        "        \"cognitive_load_assessment\": cognitive_load_b\n",
        "    }\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "8qy5mHNPT2UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0e3070-b5d1-4482-e838-ea3f1d828114"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing process_c.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from process_a import process_a1, process_a2\n",
        "from process_b import process_b\n",
        "from process_c import process_c\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze_image(image_file: UploadFile = File(...), heatmap_file: UploadFile = File(...)):\n",
        "    image_bytes = await image_file.read()\n",
        "    heatmap_bytes = await heatmap_file.read()\n",
        "\n",
        "    # Process A1\n",
        "    description_a1 = process_a1(image_bytes)\n",
        "\n",
        "    # Process A2\n",
        "    salient_elements_a2 = process_a2(image_bytes, heatmap_bytes)\n",
        "\n",
        "    # Process B\n",
        "    cognitive_load_b = process_b(image_bytes)\n",
        "\n",
        "    # Process C\n",
        "    final_output = process_c(description_a1, salient_elements_a2, cognitive_load_b)\n",
        "\n",
        "    return JSONResponse(content=final_output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "OFqfZyvdT410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3811cfe-3b43-4dc9-e79c-79e49e341e9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4VnndSNeT-JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if necessary\n",
        "ngrok.kill()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "\n",
        "public_url = ngrok.connect(port='8000')\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the FastAPI app\n",
        "!python main.py\n"
      ],
      "metadata": {
        "id": "h-W6LCUAT_LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST \"{ngrok_url}/analyze\" -F \"image_file=@path_to_image.jpg\" -F \"heatmap_file=@path_to_heatmap.jpg\"\n"
      ],
      "metadata": {
        "id": "6t7IDy4DUD1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nu8yX8MX_3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgxMRZ-oYJEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1qz-_JgYPHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tgu3IzSOYs3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWNPIhReY-Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EF_3B1xvZJwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SeXBogwDZUGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}